{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import all libraries"
      ],
      "metadata": {
        "id": "tL2yrn0wIxMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet kiteconnect pandas matplotlib numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime,timedelta\n",
        "from kiteconnect import KiteConnect\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "drive.mount('/content/drive',force_remount = True)\n",
        "print(\"All libraries installed and imported successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB1ZiYDEI3EH",
        "outputId": "cef17668-989c-4f63-fd3b-4e0067e45f7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.5/771.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "All libraries installed and imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up API key\n"
      ],
      "metadata": {
        "id": "MwiugCQwJ2zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"8re7mjcm2btaozwf\"\n",
        "API_SECRET = \"fw8gm7wfeclcic9rlkp0tbzx4h2ss2n1\"\n",
        "REDIRECT_URL = \"https://example.com\"\n",
        "\n",
        "\n",
        "kite = KiteConnect(api_key=API_KEY)\n",
        "\n",
        "\n",
        "login_url = kite.login_url()\n",
        "print(f\"Login to your Kite account using this URL: {login_url}\")\n",
        "\n",
        "\n",
        "request_token = input(\"Paste the request token from the URL here: \")\n",
        "\n",
        "\n",
        "try:\n",
        "    data = kite.generate_session(request_token=request_token, api_secret=API_SECRET)\n",
        "    access_token = data[\"access_token\"]\n",
        "    print(f\"Access Token: {access_token}\")\n",
        "\n",
        "\n",
        "    kite.set_access_token(access_token)\n",
        "    print(\"Kite Connect is now initialized and ready to use!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating access token: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f2NBZErJ4CG",
        "outputId": "5323d84a-d52a-4550-fb0d-1e8e5f80b153"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login to your Kite account using this URL: https://kite.zerodha.com/connect/login?api_key=8re7mjcm2btaozwf&v=3\n",
            "Paste the request token from the URL here: qxq4aVIgBISXhS87fpLYzGzSqIcpy23G\n",
            "Access Token: IZN1YwBYTlSfIPlJ94raX7V4q5s6DT5p\n",
            "Kite Connect is now initialized and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching NIFTY 50 instrument token\n"
      ],
      "metadata": {
        "id": "sblDPMbeKG63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def get_instrument_token(kite, tradingsymbol=\"NIFTY\", exchange=\"NSE\"):\n",
        "    try:\n",
        "        # Fetch all instruments for the given exchange\n",
        "        instruments = kite.instruments(exchange)\n",
        "\n",
        "        # Loop through the instruments and find the matching trading symbol\n",
        "        for instrument in instruments:\n",
        "            if instrument['tradingsymbol'] == tradingsymbol:\n",
        "                print(f\"Instrument Token for {tradingsymbol}: {instrument['instrument_token']}\")\n",
        "                return instrument['instrument_token']\n",
        "\n",
        "        print(f\"Instrument Token for {tradingsymbol} not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching instrument token: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example Usage\n",
        "nifty50_token = get_instrument_token(kite, tradingsymbol=\"NIFTY 50\", exchange=\"NSE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9jLKF80KND8",
        "outputId": "280b6897-d911-406c-b5dd-df4c4d39cd60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instrument Token for NIFTY 50: 256265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching 1 min timeframe data in 1min csv for 2024"
      ],
      "metadata": {
        "id": "3QzVKNOkKTOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_1min_data_2024(kite, instrument_token, drive_output_dir):\n",
        "    \"\"\"\n",
        "    Fetch 1-minute OHLC data for 2024 and save it as a CSV file on Google Drive.\n",
        "\n",
        "    Args:\n",
        "        kite: An instance of the KiteConnect API.\n",
        "        instrument_token: The instrument token for which to fetch data.\n",
        "        drive_output_dir: Path to the Google Drive directory to save the file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved CSV file.\n",
        "    \"\"\"\n",
        "    # Define the start and end date for 2024\n",
        "    start_date = datetime(2024, 1, 1)\n",
        "    end_date = datetime(2024, 12, 31)\n",
        "\n",
        "    # Split the date range into chunks of 30 days (Kite API limitation)\n",
        "    date_ranges = []\n",
        "    current_start = start_date\n",
        "    while current_start < end_date:\n",
        "        current_end = min(current_start + timedelta(days=30), end_date)\n",
        "        date_ranges.append((current_start, current_end))\n",
        "        current_start = current_end + timedelta(days=1)\n",
        "\n",
        "    # Initialize an empty DataFrame to store all data\n",
        "    all_data = pd.DataFrame()\n",
        "\n",
        "    # Fetch data for each date range\n",
        "    for start, end in date_ranges:\n",
        "        print(f\"Fetching data from {start.date()} to {end.date()}...\")\n",
        "        try:\n",
        "            # Fetch historical data from Kite API\n",
        "            data = kite.historical_data(\n",
        "                instrument_token=instrument_token,\n",
        "                from_date=start.strftime(\"%Y-%m-%d\"),\n",
        "                to_date=end.strftime(\"%Y-%m-%d\"),\n",
        "                interval=\"minute\"\n",
        "            )\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Keep only OHLC columns\n",
        "            if not df.empty:\n",
        "                df = df[['date', 'open', 'high', 'low', 'close']]\n",
        "                df['date'] = pd.to_datetime(df['date'])\n",
        "                df.set_index('date', inplace=True)\n",
        "\n",
        "                # Append to the main DataFrame\n",
        "                all_data = pd.concat([all_data, df])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data from {start.date()} to {end.date()}: {e}\")\n",
        "\n",
        "    # Save the data to a CSV file in the Google Drive directory\n",
        "    if not all_data.empty:\n",
        "        file_path = os.path.join(drive_output_dir, \"1min_data_2024.csv\")\n",
        "        all_data.to_csv(file_path)\n",
        "        print(f\"Data successfully saved to {file_path}\")\n",
        "        return file_path\n",
        "    else:\n",
        "        print(\"No data fetched.\")\n",
        "        return None\n",
        "\n",
        "# Define instrument token for the desired stock/index\n",
        "instrument_token = 256265  # Replace with your desired instrument token\n",
        "\n",
        "# Define Google Drive directory to save the file\n",
        "drive_output_dir = \"/content/drive/My Drive\"\n",
        "\n",
        "# Fetch and save 2024 data\n",
        "fetch_1min_data_2024(kite, instrument_token, drive_output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "7KYtY425KV6Z",
        "outputId": "7ea76a49-f390-4a6d-c0e9-3cd75ee78bdb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from 2024-01-01 to 2024-01-31...\n",
            "Fetching data from 2024-02-01 to 2024-03-02...\n",
            "Fetching data from 2024-03-03 to 2024-04-02...\n",
            "Fetching data from 2024-04-03 to 2024-05-03...\n",
            "Fetching data from 2024-05-04 to 2024-06-03...\n",
            "Fetching data from 2024-06-04 to 2024-07-04...\n",
            "Fetching data from 2024-07-05 to 2024-08-04...\n",
            "Fetching data from 2024-08-05 to 2024-09-04...\n",
            "Fetching data from 2024-09-05 to 2024-10-05...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-43ee04e59428>:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['date'] = pd.to_datetime(df['date'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from 2024-10-06 to 2024-11-05...\n",
            "Fetching data from 2024-11-06 to 2024-12-06...\n",
            "Fetching data from 2024-12-07 to 2024-12-31...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-43ee04e59428>:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['date'] = pd.to_datetime(df['date'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch 15min timeframe data and store in different csv file\n"
      ],
      "metadata": {
        "id": "GK9F9ZdtLZVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_15min_data_2024(kite, instrument_token, drive_output_dir):\n",
        "    \"\"\"\n",
        "    Fetch 15-minute OHLC data for 2024 and save it as a CSV file on Google Drive.\n",
        "\n",
        "    Args:\n",
        "        kite: An instance of the KiteConnect API.\n",
        "        instrument_token: The instrument token for which to fetch data.\n",
        "        drive_output_dir: Path to the Google Drive directory to save the file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved CSV file.\n",
        "    \"\"\"\n",
        "    # Initialize an empty DataFrame to store all data\n",
        "    all_data = pd.DataFrame()\n",
        "\n",
        "    # Loop through each month in 2024\n",
        "    for month in range(1, 13):\n",
        "        start_date = datetime(2024, month, 1)\n",
        "        next_month = start_date + timedelta(days=32)\n",
        "        end_date = datetime(next_month.year, next_month.month, 1) - timedelta(days=1)\n",
        "\n",
        "        print(f\"Fetching data from {start_date.date()} to {end_date.date()}...\")\n",
        "\n",
        "        try:\n",
        "            # Fetch historical data for the current month\n",
        "            data = kite.historical_data(\n",
        "                instrument_token=instrument_token,\n",
        "                from_date=start_date.strftime(\"%Y-%m-%d\"),\n",
        "                to_date=end_date.strftime(\"%Y-%m-%d\"),\n",
        "                interval=\"15minute\"\n",
        "            )\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Keep only OHLC columns\n",
        "            if not df.empty:\n",
        "                df = df[['date', 'open', 'high', 'low', 'close']]\n",
        "                df['date'] = pd.to_datetime(df['date'])\n",
        "                df.set_index('date', inplace=True)\n",
        "\n",
        "                # Append to the main DataFrame\n",
        "                all_data = pd.concat([all_data, df])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data from {start_date.date()} to {end_date.date()}: {e}\")\n",
        "\n",
        "    # Save the data to a CSV file in the Google Drive directory\n",
        "    if not all_data.empty:\n",
        "        file_path = os.path.join(drive_output_dir, \"15min_data_2024.csv\")\n",
        "        all_data.to_csv(file_path)\n",
        "        print(f\"15-minute data successfully saved to {file_path}\")\n",
        "        return file_path\n",
        "    else:\n",
        "        print(\"No data fetched.\")\n",
        "        return None\n",
        "\n",
        "# Define instrument token for the desired stock/index\n",
        "instrument_token = 256265  # Replace with your desired instrument token\n",
        "\n",
        "# Define Google Drive directory to save the file\n",
        "drive_output_dir = \"/content/drive/My Drive\"\n",
        "\n",
        "# Fetch and save 15-minute timeframe data for 2024\n",
        "fetch_15min_data_2024(kite, instrument_token, drive_output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "56wnm3c1Lb06",
        "outputId": "4143485f-3633-40e4-af04-51bac0c9ca4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from 2024-01-01 to 2024-01-31...\n",
            "Fetching data from 2024-02-01 to 2024-02-29...\n",
            "Fetching data from 2024-03-01 to 2024-03-31...\n",
            "Fetching data from 2024-04-01 to 2024-04-30...\n",
            "Fetching data from 2024-05-01 to 2024-05-31...\n",
            "Fetching data from 2024-06-01 to 2024-06-30...\n",
            "Fetching data from 2024-07-01 to 2024-07-31...\n",
            "Fetching data from 2024-08-01 to 2024-08-31...\n",
            "Fetching data from 2024-09-01 to 2024-09-30...\n",
            "Fetching data from 2024-10-01 to 2024-10-31...\n",
            "Fetching data from 2024-11-01 to 2024-11-30...\n",
            "Fetching data from 2024-12-01 to 2024-12-31...\n",
            "15-minute data successfully saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the HL/2 (high-low midpoint)\n"
      ],
      "metadata": {
        "id": "dJagt_4oMG5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def add_hl2_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate HL2 (High-Low Midpoint) and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Calculate HL2\n",
        "        df['hl2'] = (df['high'] + df['low']) / 2\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"HL2 column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding HL2: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add HL2 to the CSV file\n",
        "add_hl2_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a3PXpRUzMKYa",
        "outputId": "049f6059-6325-4caa-c8de-f1268e0cb2ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HL2 column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate ATR for supertend channel"
      ],
      "metadata": {
        "id": "Bn1TOgEMMY7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_atr_to_csv_rma(file_path, length=10, multiplier=3):\n",
        "    \"\"\"\n",
        "    Calculate ATR (Average True Range) using RMA (Wilder's Moving Average)\n",
        "    and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "        length (int): Lookback period for ATR calculation (default: 10).\n",
        "        multiplier (float): Multiplier for ATR (default: 3).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Initialize columns for True Range and ATR\n",
        "        df['true_range'] = 0.0\n",
        "        df['atr'] = 0.0\n",
        "\n",
        "        # Calculate True Range (TR)\n",
        "        for i in range(1, len(df)):\n",
        "            high = df.iloc[i]['high']\n",
        "            low = df.iloc[i]['low']\n",
        "            prev_close = df.iloc[i - 1]['close']\n",
        "\n",
        "            tr = max(\n",
        "                high - low,  # High - Low\n",
        "                abs(high - prev_close),  # High - Previous Close\n",
        "                abs(low - prev_close)  # Low - Previous Close\n",
        "            )\n",
        "            df.at[df.index[i], 'true_range'] = tr\n",
        "\n",
        "        # Calculate ATR using RMA\n",
        "        for i in range(len(df)):\n",
        "            if i == 0:\n",
        "                # Set initial ATR as the simple average of the first 'length' TR values\n",
        "                df.at[df.index[i], 'atr'] = 0.0\n",
        "            elif i < length:\n",
        "                # Simple average for the first 'length' rows\n",
        "                df.at[df.index[i], 'atr'] = df['true_range'][:i+1].mean()\n",
        "            else:\n",
        "                # Wilder's RMA formula\n",
        "                prev_atr = df.iloc[i - 1]['atr']\n",
        "                tr = df.iloc[i]['true_range']\n",
        "                df.at[df.index[i], 'atr'] = ((prev_atr * (length - 1)) + tr) / length\n",
        "\n",
        "        # Apply the multiplier to scale the ATR (if needed)\n",
        "        df['atr'] *= multiplier\n",
        "\n",
        "        # Drop the intermediate 'true_range' column (optional)\n",
        "        df.drop(columns=['true_range'], inplace=True)\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"ATR (RMA-based) column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding ATR: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add ATR to the CSV file\n",
        "add_atr_to_csv_rma(file_path, length=10, multiplier=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w3fyuBKCMboM",
        "outputId": "c93b0aba-98c3-4acd-d3d0-e79e6b41ceb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATR (RMA-based) column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate initial upper band"
      ],
      "metadata": {
        "id": "gGkF76S6MhEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_initial_upper_band_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Initial Upper Band and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'hl2' not in df.columns or 'atr' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'hl2' or 'atr' not found in the CSV file. Make sure HL2 and ATR are calculated first.\")\n",
        "\n",
        "        # Calculate Initial Upper Band\n",
        "        df['initial_upper_band'] = df['hl2'] + df['atr']\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Initial Upper Band column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Initial Upper Band: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Initial Upper Band to the CSV file\n",
        "add_initial_upper_band_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4Gcu59kOMiwA",
        "outputId": "25db69c7-88a1-4577-f622-d6939dd46dcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Upper Band column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate inital lower band"
      ],
      "metadata": {
        "id": "NuolN_1jMqlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_initial_lower_band_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Initial Lower Band and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'hl2' not in df.columns or 'atr' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'hl2' or 'atr' not found in the CSV file. Make sure HL2 and ATR are calculated first.\")\n",
        "\n",
        "        # Calculate Initial Lower Band\n",
        "        df['initial_lower_band'] = df['hl2'] - df['atr']\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Initial Lower Band column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Initial Lower Band: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Initial Lower Band to the CSV file\n",
        "add_initial_lower_band_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Nt9nfDt6Mx2t",
        "outputId": "254789ab-febf-4037-ff5b-fde057ad54e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Lower Band column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate supertrend upper band"
      ],
      "metadata": {
        "id": "shIUx8NYM4LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_supertrend_upper_band_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Dynamic Supertrend Upper Band using the previous close\n",
        "    and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'initial_upper_band' not in df.columns:\n",
        "            raise ValueError(\"Required column 'initial_upper_band' not found in the CSV file. Make sure Initial Upper Band is calculated first.\")\n",
        "\n",
        "        if 'close' not in df.columns:\n",
        "            raise ValueError(\"Required column 'close' not found in the CSV file. Ensure that the CSV contains OHLC data.\")\n",
        "\n",
        "        # Initialize the Dynamic Supertrend Upper Band column\n",
        "        df['supertrend_upper'] = 0.0\n",
        "\n",
        "        # Calculate the Dynamic Supertrend Upper Band\n",
        "        for i in range(len(df)):\n",
        "            if i == 0:\n",
        "                # For the first row, set the Supertrend Upper Band to the Initial Upper Band\n",
        "                df.at[df.index[i], 'supertrend_upper'] = df.at[df.index[i], 'initial_upper_band']\n",
        "            else:\n",
        "                prev_supertrend_upper = df.at[df.index[i - 1], 'supertrend_upper']\n",
        "                initial_upper = df.at[df.index[i], 'initial_upper_band']\n",
        "                prev_close = df.at[df.index[i - 1], 'close']  # Use the previous close price\n",
        "\n",
        "                # Apply the logic for dynamic adjustment\n",
        "                if prev_close < prev_supertrend_upper:\n",
        "                    df.at[df.index[i], 'supertrend_upper'] = min(initial_upper, prev_supertrend_upper)\n",
        "                else:\n",
        "                    df.at[df.index[i], 'supertrend_upper'] = initial_upper\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Dynamic Supertrend Upper Band column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Dynamic Supertrend Upper Band: {e}\")\n",
        "        return None\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Dynamic Supertrend Upper Band to the CSV file\n",
        "add_supertrend_upper_band_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "YLT_Wm6mM78C",
        "outputId": "207ce4e7-9bf9-45f2-c0de-501202beadad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic Supertrend Upper Band column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate supertrend lower band"
      ],
      "metadata": {
        "id": "xi_jFrQnORvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_supertrend_lower_band_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Dynamic Supertrend Lower Band using the previous close\n",
        "    and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'initial_lower_band' not in df.columns:\n",
        "            raise ValueError(\"Required column 'initial_lower_band' not found in the CSV file. Make sure Initial Lower Band is calculated first.\")\n",
        "\n",
        "        if 'close' not in df.columns:\n",
        "            raise ValueError(\"Required column 'close' not found in the CSV file. Ensure that the CSV contains OHLC data.\")\n",
        "\n",
        "        # Initialize the Dynamic Supertrend Lower Band column\n",
        "        df['supertrend_lower'] = 0.0\n",
        "\n",
        "        # Calculate the Dynamic Supertrend Lower Band\n",
        "        for i in range(len(df)):\n",
        "            if i == 0:\n",
        "                # For the first row, set the Supertrend Lower Band to the Initial Lower Band\n",
        "                df.at[df.index[i], 'supertrend_lower'] = df.at[df.index[i], 'initial_lower_band']\n",
        "            else:\n",
        "                prev_supertrend_lower = df.at[df.index[i - 1], 'supertrend_lower']\n",
        "                initial_lower = df.at[df.index[i], 'initial_lower_band']\n",
        "                prev_close = df.at[df.index[i - 1], 'close']  # Use the previous close price\n",
        "\n",
        "                # Apply the logic for dynamic adjustment\n",
        "                if prev_close >= prev_supertrend_lower:\n",
        "                    df.at[df.index[i], 'supertrend_lower'] = max(initial_lower, prev_supertrend_lower)\n",
        "                else:\n",
        "                    df.at[df.index[i], 'supertrend_lower'] = initial_lower\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Dynamic Supertrend Lower Band column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Dynamic Supertrend Lower Band: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Dynamic Supertrend Lower Band to the CSV file\n",
        "add_supertrend_lower_band_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qcIaUFijOTxQ",
        "outputId": "c8aa4a10-dab7-449a-ab1c-6e7548dfadef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic Supertrend Lower Band column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate oscillation state"
      ],
      "metadata": {
        "id": "b9vCy5QwOX-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_os_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Oscillation State (os) using current row data\n",
        "    and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'supertrend_upper' not in df.columns or 'supertrend_lower' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'supertrend_upper' or 'supertrend_lower' not found in the CSV file. Make sure Dynamic Supertrend Bands are calculated first.\")\n",
        "\n",
        "        if 'close' not in df.columns:\n",
        "            raise ValueError(\"Required column 'close' not found in the CSV file. Ensure that the CSV contains OHLC data.\")\n",
        "\n",
        "        # Initialize the Oscillation State column\n",
        "        df['os'] = 0  # Default to bearish\n",
        "\n",
        "        # Calculate the Oscillation State\n",
        "        for i in range(len(df)):\n",
        "            close = df.at[df.index[i], 'close']\n",
        "            upper_band = df.at[df.index[i], 'supertrend_upper']\n",
        "            lower_band = df.at[df.index[i], 'supertrend_lower']\n",
        "\n",
        "            # Compare the current close with the current bands\n",
        "            if close > upper_band:\n",
        "                df.at[df.index[i], 'os'] = 1  # Bullish\n",
        "            elif close < lower_band:\n",
        "                df.at[df.index[i], 'os'] = 0  # Bearish\n",
        "            else:\n",
        "                if i > 0:\n",
        "                    # Retain the previous state if close is between the bands\n",
        "                    df.at[df.index[i], 'os'] = df.at[df.index[i - 1], 'os']\n",
        "                else:\n",
        "                    # For the first row, default to bearish if between bands\n",
        "                    df.at[df.index[i], 'os'] = 0\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Oscillation State column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Oscillation State: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Oscillation State to the CSV file\n",
        "add_os_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-Z03IzzYOaAL",
        "outputId": "19f3c7ce-a328-4911-a683-30fd5888f834"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oscillation State column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate supertrend pivot"
      ],
      "metadata": {
        "id": "Qtrz4cDpOesD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_supertrend_pivot_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Supertrend Pivot (spt) and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'os' not in df.columns:\n",
        "            raise ValueError(\"Required column 'os' not found in the CSV file. Make sure Oscillation State is calculated first.\")\n",
        "\n",
        "        if 'supertrend_upper' not in df.columns or 'supertrend_lower' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'supertrend_upper' or 'supertrend_lower' not found in the CSV file. Make sure Dynamic Supertrend Bands are calculated first.\")\n",
        "\n",
        "        # Initialize the Supertrend Pivot column\n",
        "        df['spt'] = 0.0\n",
        "\n",
        "        # Calculate the Supertrend Pivot\n",
        "        for i in range(len(df)):\n",
        "            os = df.at[df.index[i], 'os']\n",
        "            if os == 1:\n",
        "                # Bullish: Use the Supertrend Lower Band as the pivot\n",
        "                df.at[df.index[i], 'spt'] = df.at[df.index[i], 'supertrend_lower']\n",
        "            else:\n",
        "                # Bearish: Use the Supertrend Upper Band as the pivot\n",
        "                df.at[df.index[i], 'spt'] = df.at[df.index[i], 'supertrend_upper']\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Supertrend Pivot column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Supertrend Pivot: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Supertrend Pivot to the CSV file\n",
        "add_supertrend_pivot_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UWbcIdCwOg-U",
        "outputId": "a9f6fb52-c90b-4e7f-9b3d-c6a3fd87ea1a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supertrend Pivot column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate max channel"
      ],
      "metadata": {
        "id": "aMefVkF5Oki1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_max_channel_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Max Channel incorporating the Supertrend Pivot (spt)\n",
        "    and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'os' not in df.columns or 'close' not in df.columns or 'spt' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'os', 'close', or 'spt' not found in the CSV file. Make sure Oscillation State and Supertrend Pivot are calculated first.\")\n",
        "\n",
        "        # Initialize the Max Channel column\n",
        "        df['max_channel'] = 0.0\n",
        "\n",
        "        # Calculate the Max Channel\n",
        "        for i in range(len(df)):\n",
        "            close = df.at[df.index[i], 'close']\n",
        "            os = df.at[df.index[i], 'os']\n",
        "            spt = df.at[df.index[i], 'spt']\n",
        "\n",
        "            if i == 0:\n",
        "                # Initialize Max Channel for the first row\n",
        "                df.at[df.index[i], 'max_channel'] = close\n",
        "            else:\n",
        "                prev_max_channel = df.at[df.index[i - 1], 'max_channel']\n",
        "                prev_os = df.at[df.index[i - 1], 'os']\n",
        "\n",
        "                if close > spt:  # Price crosses the Supertrend Pivot\n",
        "                    df.at[df.index[i], 'max_channel'] = max(prev_max_channel, close)\n",
        "                elif os == 1:  # Bullish trend\n",
        "                    df.at[df.index[i], 'max_channel'] = max(close, prev_max_channel)\n",
        "                else:  # Bearish trend\n",
        "                    df.at[df.index[i], 'max_channel'] = min(spt, prev_max_channel)\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Max Channel column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Max Channel: {e}\")\n",
        "        return None\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Max Channel to the CSV file\n",
        "add_max_channel_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_BBwwT5POo9a",
        "outputId": "31402191-c675-4508-b921-68d5b6a0a64b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Channel column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate min channel"
      ],
      "metadata": {
        "id": "jF21fheLPASj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_min_channel_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Min Channel incorporating the Supertrend Pivot (spt)\n",
        "    and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'os' not in df.columns or 'close' not in df.columns or 'spt' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'os', 'close', or 'spt' not found in the CSV file. Make sure Oscillation State and Supertrend Pivot are calculated first.\")\n",
        "\n",
        "        # Initialize the Min Channel column\n",
        "        df['min_channel'] = 0.0\n",
        "\n",
        "        # Calculate the Min Channel\n",
        "        for i in range(len(df)):\n",
        "            close = df.at[df.index[i], 'close']\n",
        "            os = df.at[df.index[i], 'os']\n",
        "            spt = df.at[df.index[i], 'spt']\n",
        "\n",
        "            if i == 0:\n",
        "                # Initialize Min Channel for the first row\n",
        "                df.at[df.index[i], 'min_channel'] = close\n",
        "            else:\n",
        "                prev_min_channel = df.at[df.index[i - 1], 'min_channel']\n",
        "                prev_os = df.at[df.index[i - 1], 'os']\n",
        "\n",
        "                if close < spt:  # Price crosses the Supertrend Pivot\n",
        "                    df.at[df.index[i], 'min_channel'] = min(prev_min_channel, close)\n",
        "                elif os == 0:  # Bearish trend\n",
        "                    df.at[df.index[i], 'min_channel'] = min(close, prev_min_channel)\n",
        "                else:  # Bullish trend\n",
        "                    df.at[df.index[i], 'min_channel'] = max(spt, prev_min_channel)\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Min Channel column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Min Channel: {e}\")\n",
        "        return None\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Min Channel to the CSV file\n",
        "add_min_channel_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yUaWbqy8PBiL",
        "outputId": "44d0bb8e-0d5c-40bc-c6b4-db602e681a71"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min Channel column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate supertrend average channel"
      ],
      "metadata": {
        "id": "l8yQEymnPGP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_supertrend_avg_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the Supertrend Average Channel using Max and Min Channels,\n",
        "    and add it as a column to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure required columns are present\n",
        "        if 'max_channel' not in df.columns or 'min_channel' not in df.columns:\n",
        "            raise ValueError(\"Required columns 'max_channel' or 'min_channel' not found in the CSV file. Make sure Max and Min Channels are calculated first.\")\n",
        "\n",
        "        # Calculate the Supertrend Average Channel\n",
        "        df['supertrend_avg'] = (df['max_channel'] + df['min_channel']) / 2\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"Supertrend Average Channel column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding Supertrend Average Channel: {e}\")\n",
        "        return None\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add Supertrend Average Channel to the CSV file\n",
        "add_supertrend_avg_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CHjX-8A3PIiR",
        "outputId": "f7e77111-6e20-4e33-d9cf-218d453148d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supertrend Average Channel column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate 5WMA for 15min timeframe"
      ],
      "metadata": {
        "id": "dtlF0LDA5djY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def add_5wma_to_csv(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the 5-period Weighted Moving Average (5WMA) and add it as a column to the CSV file.\n",
        "    For the first 4 rows where 5WMA cannot be computed, use the 'close' price as the value.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Ensure the 'close' column is present\n",
        "        if 'close' not in df.columns:\n",
        "            raise ValueError(\"Required column 'close' not found in the CSV file. Please ensure the data contains a 'close' column.\")\n",
        "\n",
        "        # Calculate the 5-period Weighted Moving Average (5WMA)\n",
        "        weights = [1, 2, 3, 4, 5]\n",
        "        df['5wma'] = df['close'].rolling(window=5).apply(lambda x: sum(w * c for w, c in zip(weights, x)) / sum(weights), raw=True)\n",
        "\n",
        "        # Replace the first 4 NaN values with the corresponding close price\n",
        "        df['5wma'].fillna(df['close'], inplace=True)\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"5WMA column added and saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding 5WMA: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to the 15-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add 5WMA to the CSV file\n",
        "add_5wma_to_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "X_gZvMdG5iN9",
        "outputId": "032a27b7-30d9-426b-95d3-7bf8c0112534"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-aed1e0b89eec>:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['5wma'].fillna(df['close'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5WMA column added and saved to /content/drive/My Drive/15min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/15min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add new cols in 1min csv file"
      ],
      "metadata": {
        "id": "vJK3CcNDPURi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_empty_columns_to_1min_csv(file_path):\n",
        "    \"\"\"\n",
        "    Add empty columns for 15-minute data to the 1-minute CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the 1-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute CSV file\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
        "\n",
        "        # Define the new columns to add\n",
        "        new_columns = [\n",
        "            '15min_open', '15min_high', '15min_low', '15min_close',\n",
        "            '15min_max_channel', '15min_min_channel', '15min_supertrend_avg','15min_5wma'\n",
        "        ]\n",
        "\n",
        "        # Add each column with default values of NaN\n",
        "        for col in new_columns:\n",
        "            if col not in df.columns:  # Avoid duplicate columns\n",
        "                df[col] = float('nan')\n",
        "\n",
        "        # Save the updated DataFrame back to the CSV file\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"New columns added to {file_path}: {', '.join(new_columns)}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error while adding empty columns: {e}\")\n",
        "        return None\n",
        "# Path to the 1-minute CSV file\n",
        "file_path = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this with the actual file path\n",
        "\n",
        "# Add empty columns for 15-minute data\n",
        "add_empty_columns_to_1min_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "QdcCWE2nPWFz",
        "outputId": "3b02b133-35dc-45e9-9da0-4971ffbd3c7a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New columns added to /content/drive/My Drive/1min_data_2024.csv: 15min_open, 15min_high, 15min_low, 15min_close, 15min_max_channel, 15min_min_channel, 15min_supertrend_avg, 15min_5wma\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min data to 1min csv\n"
      ],
      "metadata": {
        "id": "zDzVg8sj9uRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_open_to_1min(file_1min, file_15min):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute open values to the 1-minute CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_open if it doesn't exist\n",
        "        if '15min_open' not in df_1min.columns:\n",
        "            df_1min['15min_open'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the open value from the 15-minute row\n",
        "            open_15min = df_15min.iloc[i]['open']  # Assuming the column is named 'open'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_open'] = open_15min\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_open column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_open column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute open values\n",
        "copy_15min_open_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "bIDg97cY9xbv",
        "outputId": "60207a9a-3454-4ae6-be63-54d86f13f936"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_open column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min close to 1min csv"
      ],
      "metadata": {
        "id": "NLaIvTfG91sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_close_to_1min(file_1min, file_15min, trading_start_time=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute close values to the 1-minute CSV file and omits the 9:14 AM timestamp\n",
        "    added erroneously after the last trading day.\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "        trading_start_time (str): Start time of the trading session (HH:MM:SS).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_close if it doesn't exist\n",
        "        if '15min_close' not in df_1min.columns:\n",
        "            df_1min['15min_close'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the close value from the 15-minute row\n",
        "            close_15min = df_15min.iloc[i]['close']  # Assuming the column is named 'close'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_close'] = close_15min\n",
        "\n",
        "        # Filter out any rows with the 9:14 AM timestamp that appear erroneously\n",
        "        df_1min = df_1min[~((df_1min.index.time == pd.Timestamp(trading_start_time).time()) & (df_1min.index > df_15min.index[-1]))]\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_close column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_close column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute close values\n",
        "copy_15min_close_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "QN_E0V_U96Yr",
        "outputId": "cb2ee383-54ae-41ba-8cab-7c810adc6559"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_close column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min low"
      ],
      "metadata": {
        "id": "Pas7HjBD-LAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_low_to_1min(file_1min, file_15min, trading_start_time=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute low values to the 1-minute CSV file\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "        trading_start_time (str): Start time of the trading session (HH:MM:SS).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_low if it doesn't exist\n",
        "        if '15min_low' not in df_1min.columns:\n",
        "            df_1min['15min_low'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the low value from the 15-minute row\n",
        "            low_15min = df_15min.iloc[i]['low']  # Assuming the column is named 'low'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_low'] = low_15min\n",
        "\n",
        "        # Filter out any rows with the 9:14 AM timestamp that appear erroneously\n",
        "        df_1min = df_1min[~((df_1min.index.time == pd.Timestamp(trading_start_time).time()) & (df_1min.index > df_15min.index[-1]))]\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_low column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_low column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute low values\n",
        "copy_15min_low_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "wAKa81SO-MGe",
        "outputId": "add9d1ba-e45c-4ae3-bf60-c84e4c187148"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_low column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min high"
      ],
      "metadata": {
        "id": "Tu9L31dl-TuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_high_to_1min(file_1min, file_15min, trading_start_time=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute high values to the 1-minute CSV file\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "        trading_start_time (str): Start time of the trading session (HH:MM:SS).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_high if it doesn't exist\n",
        "        if '15min_high' not in df_1min.columns:\n",
        "            df_1min['15min_high'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the high value from the 15-minute row\n",
        "            high_15min = df_15min.iloc[i]['high']  # Assuming the column is named 'high'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_high'] = high_15min\n",
        "\n",
        "        # Filter out any rows with the 9:14 AM timestamp that appear erroneously\n",
        "        df_1min = df_1min[~((df_1min.index.time == pd.Timestamp(trading_start_time).time()) & (df_1min.index > df_15min.index[-1]))]\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_high column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_high column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute high values\n",
        "copy_15min_high_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "zaFyU-3v-YtC",
        "outputId": "f6dc3f2d-3726-4271-c506-a20d8ed974db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_high column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min max channel"
      ],
      "metadata": {
        "id": "MDD-LXFq-cdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_max_channel_to_1min(file_1min, file_15min, trading_start_time=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute max_channel values to the 1-minute CSV file\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "        trading_start_time (str): Start time of the trading session (HH:MM:SS).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_max_channel if it doesn't exist\n",
        "        if '15min_max_channel' not in df_1min.columns:\n",
        "            df_1min['15min_max_channel'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the max_channel value from the 15-minute row\n",
        "            max_channel_15min = df_15min.iloc[i]['max_channel']  # Assuming the column is named 'max_channel'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_max_channel'] = max_channel_15min\n",
        "\n",
        "        # Filter out any rows with the 9:14 AM timestamp that appear erroneously\n",
        "        df_1min = df_1min[~((df_1min.index.time == pd.Timestamp(trading_start_time).time()) & (df_1min.index > df_15min.index[-1]))]\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_max_channel column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_max_channel column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute max_channel values\n",
        "copy_15min_max_channel_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "OaDF4YKO-fiv",
        "outputId": "b5728a18-feef-4f97-f0a1-271b94baa839"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_max_channel column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min min channel"
      ],
      "metadata": {
        "id": "EaZqOO7Y-qrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_min_channel_to_1min(file_1min, file_15min, trading_start_time=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute min_channel values to the 1-minute CSV file\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "        trading_start_time (str): Start time of the trading session (HH:MM:SS).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_min_channel if it doesn't exist\n",
        "        if '15min_min_channel' not in df_1min.columns:\n",
        "            df_1min['15min_min_channel'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the min_channel value from the 15-minute row\n",
        "            min_channel_15min = df_15min.iloc[i]['min_channel']  # Assuming the column is named 'min_channel'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_min_channel'] = min_channel_15min\n",
        "\n",
        "        # Filter out any rows with the 9:14 AM timestamp that appear erroneously\n",
        "        df_1min = df_1min[~((df_1min.index.time == pd.Timestamp(trading_start_time).time()) & (df_1min.index > df_15min.index[-1]))]\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_min_channel column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_min_channel column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute min_channel values\n",
        "copy_15min_min_channel_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "jvUwQA5d-r_W",
        "outputId": "0228729c-fd88-4c53-b6b8-3dbb146e3cf0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_min_channel column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min supertrend avg channel"
      ],
      "metadata": {
        "id": "Zd6Un2qr-yeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_supertrend_avg_to_1min(file_1min, file_15min, trading_start_time=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute supertrend_avg values to the 1-minute CSV file\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "        trading_start_time (str): Start time of the trading session (HH:MM:SS).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_supertrend_avg if it doesn't exist\n",
        "        if '15min_supertrend_avg' not in df_1min.columns:\n",
        "            df_1min['15min_supertrend_avg'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the supertrend_avg value from the 15-minute row\n",
        "            supertrend_avg_15min = df_15min.iloc[i]['supertrend_avg']  # Assuming the column is named 'supertrend_avg'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_supertrend_avg'] = supertrend_avg_15min\n",
        "\n",
        "        # Filter out any rows with the 9:14 AM timestamp that appear erroneously\n",
        "        df_1min = df_1min[~((df_1min.index.time == pd.Timestamp(trading_start_time).time()) & (df_1min.index > df_15min.index[-1]))]\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_supertrend_avg column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_supertrend_avg column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute supertrend_avg values\n",
        "copy_15min_supertrend_avg_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "gULAPvyM-0NL",
        "outputId": "b25cce67-950c-43cb-f0ae-e4e004c99387"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_supertrend_avg column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy 15min 5WMA data to 1min csv file"
      ],
      "metadata": {
        "id": "lf9jphD1-7ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def copy_15min_5wma_to_1min(file_1min, file_15min, trading_start_time=\"09:15:00\"):\n",
        "    \"\"\"\n",
        "    Copies the 15-minute 5WMA values to the 1-minute CSV file\n",
        "\n",
        "    Args:\n",
        "        file_1min (str): Path to the 1-minute CSV file.\n",
        "        file_15min (str): Path to the 15-minute CSV file.\n",
        "        trading_start_time (str): Start time of the trading session (HH:MM:SS).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the updated 1-minute CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the 1-minute and 15-minute CSV files\n",
        "        df_1min = pd.read_csv(file_1min, header=0)\n",
        "        df_15min = pd.read_csv(file_15min, header=0)\n",
        "\n",
        "        # Convert the first column in both files to datetime\n",
        "        df_1min.iloc[:, 0] = pd.to_datetime(df_1min.iloc[:, 0], errors='coerce')\n",
        "        df_15min.iloc[:, 0] = pd.to_datetime(df_15min.iloc[:, 0], errors='coerce')\n",
        "\n",
        "        # Set the timestamp columns as the index\n",
        "        df_1min.set_index(df_1min.columns[0], inplace=True)\n",
        "        df_15min.set_index(df_15min.columns[0], inplace=True)\n",
        "\n",
        "        # Add a new column in the 1-minute file for 15min_supertrend_avg if it doesn't exist\n",
        "        if '15min_5wma' not in df_1min.columns:\n",
        "            df_1min['15min_5wma'] = None\n",
        "\n",
        "        # Iterate through the 15-minute data\n",
        "        for i in range(len(df_15min)):\n",
        "            # Get the current and next timestamps\n",
        "            current_time = df_15min.index[i]\n",
        "            next_time = (\n",
        "                df_15min.index[i + 1] if i + 1 < len(df_15min) else current_time + pd.Timedelta(minutes=15)\n",
        "            )\n",
        "\n",
        "            # Get the 5WMA value from the 15-minute row\n",
        "            wma_15min = df_15min.iloc[i]['5wma']  # Assuming the column is named '5wma'\n",
        "\n",
        "            # Update all rows in the 1-minute file within the current 15-minute range\n",
        "            df_1min.loc[current_time:next_time - pd.Timedelta(minutes=1), '15min_5wma'] = wma_15min\n",
        "\n",
        "        # Filter out any rows with the 9:14 AM timestamp that appear erroneously\n",
        "        df_1min = df_1min[~((df_1min.index.time == pd.Timestamp(trading_start_time).time()) & (df_1min.index > df_15min.index[-1]))]\n",
        "\n",
        "        # Save the updated DataFrame back to the 1-minute CSV file\n",
        "        df_1min.to_csv(file_1min)\n",
        "        print(f\"15min_5wma column copied and saved to {file_1min}\")\n",
        "        return file_1min\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while copying 15min_5wma column: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the CSV files\n",
        "file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update this path\n",
        "file_15min = \"/content/drive/My Drive/15min_data_2024.csv\"  # Update this path\n",
        "\n",
        "# Call the function to copy the 15-minute 5wma values\n",
        "copy_15min_5wma_to_1min(file_1min, file_15min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "sHS4Frda--Gb",
        "outputId": "8d3993c1-17a2-4bb7-f766-95ee6f692ff5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
            "  return Index(sequences[0], name=names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15min_5wma column copied and saved to /content/drive/My Drive/1min_data_2024.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/1min_data_2024.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check CE side conditions\n"
      ],
      "metadata": {
        "id": "9ItT26lUdMoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_ce(data, row):\n",
        "    \"\"\"\n",
        "    Checks if the CE strategy conditions are satisfied for the given row.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The 1-minute CSV data.\n",
        "        row (int): The current row to check the CE strategy conditions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if CE alert and trigger conditions are satisfied, else False.\n",
        "    \"\"\"\n",
        "    # Get the alert candle (current row)\n",
        "    alert = data.iloc[row]\n",
        "\n",
        "\n",
        "    # Condition 1: 15min high > 15min min channel for the alert candle\n",
        "    if not (alert['15min_high']>= alert['15min_min_channel']):\n",
        "        return False\n",
        "\n",
        "    # Condition 2: 15min low < 15min min channel for the alert candle\n",
        "    if not (\n",
        "        alert['15min_low']<= alert['15min_min_channel']\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    #Condition 3: 15min 5WMA > 15min high (gap)\n",
        "    if not (\n",
        "        alert['15min_5wma'] > alert['15min_high']\n",
        "    ):\n",
        "        return False\n",
        "    # Condition 4: Trigger condition - within the next 30 rows\n",
        "    for i in range(15, 31):\n",
        "        if row + i >= len(data):  # Ensure within bounds\n",
        "            break\n",
        "        check = data.iloc[row + i]\n",
        "        if ((check['close'] > alert['15min_high']) and(check['15min_5wma'] > alert['15min_high'])):\n",
        "            return True\n",
        "\n",
        "    # If all conditions fail\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "j-ej2OZrdQuh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check PE side conditions"
      ],
      "metadata": {
        "id": "LLhatbz3dRFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_pe(data, row):\n",
        "    \"\"\"\n",
        "    Checks if the PE strategy conditions are satisfied for the given row.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The 1-minute CSV data.\n",
        "        row (int): The current row to check the PE strategy conditions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if PE alert and trigger conditions are satisfied, else False.\n",
        "    \"\"\"\n",
        "    # Get the alert candle (current row)\n",
        "    alert = data.iloc[row]\n",
        "\n",
        "\n",
        "    # Condition 1: 15min high > 15min max channel for the alert candle\n",
        "    if not (alert['15min_high']>= alert['15min_max_channel']):\n",
        "        return False\n",
        "\n",
        "    # Condition 2: 15min low < 15min max channel for the alert candle\n",
        "    if not (\n",
        "        alert['15min_low']<= alert['15min_max_channel']\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    #Condition 3: 15min 5WMA < 15min low (gap)\n",
        "    if not (\n",
        "        alert['15min_5wma'] < alert['15min_low']\n",
        "    ):\n",
        "        return False\n",
        "    # Condition 4: Trigger condition - within the next 30 rows\n",
        "    for i in range(15, 31):\n",
        "        if row + i >= len(data):  # Ensure within bounds\n",
        "            break\n",
        "        check = data.iloc[row + i]\n",
        "        if ((check['close'] < alert['15min_low']) and (check['15min_5wma'] <alert['15min_low'])):\n",
        "            return True\n",
        "\n",
        "    # If all conditions fail\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "m_TYyX-cdSp1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute CE strategy"
      ],
      "metadata": {
        "id": "w96sOFA9523F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_ce(data, row):\n",
        "    \"\"\"\n",
        "    Executes the CE strategy when the alert and trigger conditions are satisfied.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The 1-minute CSV data.\n",
        "        row (int): The starting row where the CE conditions are checked.\n",
        "\n",
        "    Returns:\n",
        "        dict: Contains profit, rows used, original SL, trailing SL, entry & exit details, and trade result status.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    alert = data.iloc[row]\n",
        "    rows_used = 15\n",
        "\n",
        "    # Find the trigger row (guaranteed by check_ce)\n",
        "    trigger = None\n",
        "    for i in range(15, 31):\n",
        "        if row + i >= len(data):  # Ensure within bounds\n",
        "            break\n",
        "        check = data.iloc[row + i]\n",
        "        rows_used += 1\n",
        "        if ((check['close'] > alert['15min_high']) and (check['15min_5wma'] > alert['15min_high'])):\n",
        "            trigger = row + i\n",
        "            break\n",
        "\n",
        "    if trigger is None:\n",
        "        return {\n",
        "            'profit': 0,\n",
        "            'rows_used': rows_used,\n",
        "            'original_sl': None,\n",
        "            'trail_sl': None,\n",
        "            'status': 'no_trade',\n",
        "            'entry_datetime': None,\n",
        "            'exit_datetime': None,\n",
        "            'entry_price': None,\n",
        "            'exit_price': None\n",
        "        }\n",
        "\n",
        "    # Trade execution\n",
        "    trigger_row = data.iloc[trigger + 1]\n",
        "    entry_price = trigger_row['open']\n",
        "    original_sl = alert['15min_min_channel']\n",
        "    trail_sl = original_sl\n",
        "    profit = 0\n",
        "    qty = 1\n",
        "    status = None\n",
        "\n",
        "    # Capture entry datetime\n",
        "    entry_datetime = trigger_row.name\n",
        "\n",
        "    for i in range(trigger + 1, len(data)):\n",
        "        candle = data.iloc[i]\n",
        "        rows_used += 1\n",
        "\n",
        "        # Stop Loss Condition\n",
        "        if candle['close'] <= original_sl:\n",
        "            profit -= qty * abs(entry_price - original_sl)\n",
        "            return {\n",
        "                'profit': profit,\n",
        "                'rows_used': rows_used,\n",
        "                'original_sl': original_sl,\n",
        "                'trail_sl': trail_sl,\n",
        "                'status': 'loss',\n",
        "                'entry_datetime': entry_datetime,\n",
        "                'exit_datetime': candle.name,\n",
        "                'entry_price': entry_price,\n",
        "                'exit_price': original_sl\n",
        "            }\n",
        "\n",
        "        # Check Target 1 (30 pts)\n",
        "        if (candle['close'] - entry_price >= 50) and (qty == 1):\n",
        "            profit += 50 * 0.25\n",
        "            qty -= 0.25\n",
        "            trail_sl = entry_price + 50\n",
        "\n",
        "        # Check Target 2 (50 pts)\n",
        "        if (candle['close'] - entry_price >= 70) and (qty == 0.75):\n",
        "            profit += 70 * 0.25\n",
        "            qty -= 0.25\n",
        "            trail_sl = entry_price + 70\n",
        "\n",
        "        # Check Target 3 (70 pts)\n",
        "        if (candle['close'] - entry_price >= 100) and (qty == 0.5):\n",
        "            profit += 100 * 0.5\n",
        "            qty = 0\n",
        "            return {\n",
        "                'profit': profit,\n",
        "                'rows_used': rows_used,\n",
        "                'original_sl': original_sl,\n",
        "                'trail_sl': trail_sl,\n",
        "                'status': 'win',\n",
        "                'entry_datetime': entry_datetime,\n",
        "                'exit_datetime': candle.name,\n",
        "                'entry_price': entry_price,\n",
        "                'exit_price': candle['close']\n",
        "            }\n",
        "\n",
        "        # Check Trailing Stop Loss\n",
        "        if candle['close'] <= trail_sl:\n",
        "            profit += (trail_sl - entry_price) * qty\n",
        "            qty = 0\n",
        "            return {\n",
        "                'profit': profit,\n",
        "                'rows_used': rows_used,\n",
        "                'original_sl': original_sl,\n",
        "                'trail_sl': trail_sl,\n",
        "                'status': 'win',\n",
        "                'entry_datetime': entry_datetime,\n",
        "                'exit_datetime': candle.name,\n",
        "                'entry_price': entry_price,\n",
        "                'exit_price': candle['close']\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        'profit': profit,\n",
        "        'rows_used': rows_used,\n",
        "        'original_sl': original_sl,\n",
        "        'trail_sl': trail_sl,\n",
        "        'status': 'win',\n",
        "        'entry_datetime': entry_datetime,\n",
        "        'exit_datetime': None,  # If no exit condition met, exit datetime is None\n",
        "        'entry_price': entry_price,\n",
        "        'exit_price': None  # No exit price if no exit condition met\n",
        "    }\n"
      ],
      "metadata": {
        "id": "1JgC1CsU54bU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute PE strategy"
      ],
      "metadata": {
        "id": "yNs6dsAs540j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_pe(data, row):\n",
        "    \"\"\"\n",
        "    Executes the PE strategy when the alert and trigger conditions are satisfied.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The 1-minute CSV data.\n",
        "        row (int): The starting row where the PE conditions are checked.\n",
        "\n",
        "    Returns:\n",
        "        dict: Contains profit, rows used, original SL, trailing SL, entry & exit details, and trade result status.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    alert = data.iloc[row]\n",
        "    rows_used = 15\n",
        "\n",
        "    # Find the trigger row (guaranteed by check_pe)\n",
        "    trigger = None\n",
        "    for i in range(15, 31):\n",
        "        if row + i >= len(data):  # Ensure within bounds\n",
        "            break\n",
        "        check = data.iloc[row + i]\n",
        "        rows_used += 1\n",
        "        if (check['close'] < alert['15min_low'] and (check['15min_5wma'] < alert['15min_low'])):\n",
        "            trigger = row + i\n",
        "            break\n",
        "\n",
        "    if trigger is None:\n",
        "        return {\n",
        "            'profit': 0,\n",
        "            'rows_used': rows_used,\n",
        "            'original_sl': None,\n",
        "            'trail_sl': None,\n",
        "            'status': 'no_trade',\n",
        "            'entry_datetime': None,\n",
        "            'exit_datetime': None,\n",
        "            'entry_price': None,\n",
        "            'exit_price': None\n",
        "        }\n",
        "\n",
        "    # Trade execution\n",
        "    trigger_row = data.iloc[trigger + 1]\n",
        "    entry_price = trigger_row['open']\n",
        "    original_sl = alert['15min_max_channel']\n",
        "    trail_sl = original_sl\n",
        "    profit = 0\n",
        "    qty = 1\n",
        "    status = None\n",
        "\n",
        "    # Capture entry datetime\n",
        "    entry_datetime = trigger_row.name\n",
        "\n",
        "    for i in range(trigger + 1, len(data)):\n",
        "        candle = data.iloc[i]\n",
        "        rows_used += 1\n",
        "\n",
        "        # Stop Loss Condition\n",
        "        if candle['close'] >= original_sl:\n",
        "            profit -= qty * abs(original_sl - entry_price)\n",
        "            return {\n",
        "                'profit': profit,\n",
        "                'rows_used': rows_used,\n",
        "                'original_sl': original_sl,\n",
        "                'trail_sl': trail_sl,\n",
        "                'status': 'loss',\n",
        "                'entry_datetime': entry_datetime,\n",
        "                'exit_datetime': candle.name,\n",
        "                'entry_price': entry_price,\n",
        "                'exit_price': original_sl\n",
        "            }\n",
        "\n",
        "        # Check Target 1 (30 pts)\n",
        "        if (entry_price - candle['close'] >= 50) and (qty == 1):\n",
        "            profit += 50 * 0.25\n",
        "            qty -= 0.25\n",
        "            trail_sl = entry_price - 50\n",
        "\n",
        "        # Check Target 2 (50 pts)\n",
        "        if (entry_price - candle['close'] >= 70) and (qty == 0.75):\n",
        "            profit += 70 * 0.25\n",
        "            qty -= 0.25\n",
        "            trail_sl = entry_price - 70\n",
        "\n",
        "        # Check Target 3 (70 pts)\n",
        "        if (entry_price - candle['close'] >= 100) and (qty == 0.5):\n",
        "            profit += 100 * 0.5\n",
        "            qty = 0\n",
        "            return {\n",
        "                'profit': profit,\n",
        "                'rows_used': rows_used,\n",
        "                'original_sl': original_sl,\n",
        "                'trail_sl': trail_sl,\n",
        "                'status': 'win',\n",
        "                'entry_datetime': entry_datetime,\n",
        "                'exit_datetime': candle.name,\n",
        "                'entry_price': entry_price,\n",
        "                'exit_price': candle['close']\n",
        "            }\n",
        "\n",
        "        # Check Trailing Stop Loss\n",
        "        if candle['close'] >= trail_sl:\n",
        "            profit += (entry_price - trail_sl) * qty\n",
        "            qty = 0\n",
        "            return {\n",
        "                'profit': profit,\n",
        "                'rows_used': rows_used,\n",
        "                'original_sl': original_sl,\n",
        "                'trail_sl': trail_sl,\n",
        "                'status': 'win',\n",
        "                'entry_datetime': entry_datetime,\n",
        "                'exit_datetime': candle.name,\n",
        "                'entry_price': entry_price,\n",
        "                'exit_price': candle['close']\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        'profit': profit,\n",
        "        'rows_used': rows_used,\n",
        "        'original_sl': original_sl,\n",
        "        'trail_sl': trail_sl,\n",
        "        'status': 'win',\n",
        "        'entry_datetime': entry_datetime,\n",
        "        'exit_datetime': None,  # If no exit condition met, exit datetime is None\n",
        "        'entry_price': entry_price,\n",
        "        'exit_price': None  # No exit price if no exit condition met\n",
        "    }\n"
      ],
      "metadata": {
        "id": "SoZH9FFA56M4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backtesting and evaluating strategy results"
      ],
      "metadata": {
        "id": "3vV9CYXS-Skq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def execute_strategy(data, trade_log_file=\"/content/drive/My Drive/trade_details.csv\"):\n",
        "    \"\"\"\n",
        "    Execute both PE and CE strategies on the 1-minute data and log trade details.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The 1-minute DataFrame containing all required data.\n",
        "        trade_log_file (str): Path to the CSV file where trade details will be logged in Google Drive.\n",
        "\n",
        "    Returns:\n",
        "        dict: Summary of backtest results including trade count, win/loss ratio, and net profit.\n",
        "    \"\"\"\n",
        "    # Initialize metrics\n",
        "    trade_count = 0\n",
        "    win_count = 0\n",
        "    loss_count = 0\n",
        "    net_profit = 0\n",
        "    win_points = 0\n",
        "    loss_points = 0\n",
        "    ce_count = 0\n",
        "    pe_count = 0\n",
        "\n",
        "    # List to store trade details\n",
        "    trade_log = []\n",
        "\n",
        "    # Start iterating through rows\n",
        "    i = 106\n",
        "    while i < len(data):\n",
        "        # Check for PE strategy\n",
        "        if check_pe(data, i):\n",
        "            pe_result = execute_pe(data, i)\n",
        "\n",
        "            # Skip if no trade occurred\n",
        "            if pe_result['status'] == 'no_trade':\n",
        "                i += 1  # Move to the next row\n",
        "                continue\n",
        "\n",
        "            # Count the trade\n",
        "            trade_count += 1\n",
        "            net_profit += pe_result['profit']\n",
        "            pe_count += 1\n",
        "\n",
        "            # Determine if it was a win or loss trade\n",
        "            if pe_result['status'] == 'win':\n",
        "                win_count += 1\n",
        "                win_points += pe_result['profit']\n",
        "            else:\n",
        "                loss_count += 1\n",
        "                loss_points += pe_result['profit']\n",
        "\n",
        "            # Log trade details\n",
        "            trade_log.append({\n",
        "                \"Trade Type\": \"PE\",\n",
        "                \"Entry Datetime\": pe_result['entry_datetime'],\n",
        "                \"Exit Datetime\": pe_result['exit_datetime'],\n",
        "                \"Entry Price\": pe_result['entry_price'],\n",
        "                \"Exit Price\": pe_result['exit_price'],\n",
        "                \"Profit Points\": pe_result['profit'],\n",
        "                \"Status\": pe_result['status']\n",
        "            })\n",
        "\n",
        "            # Skip rows used during this trade\n",
        "            i += pe_result['rows_used']\n",
        "            continue\n",
        "\n",
        "        # Check for CE strategy\n",
        "        if check_ce(data, i):\n",
        "            ce_result = execute_ce(data, i)\n",
        "\n",
        "            # Skip if no trade occurred\n",
        "            if ce_result['status'] == 'no_trade':\n",
        "                i += 1  # Move to the next row\n",
        "                continue\n",
        "\n",
        "            # Count the trade\n",
        "            trade_count += 1\n",
        "            net_profit += ce_result['profit']\n",
        "            ce_count += 1\n",
        "\n",
        "            # Determine if it was a win or loss trade\n",
        "            if ce_result['status'] == 'win':\n",
        "                win_count += 1\n",
        "                win_points += ce_result['profit']\n",
        "            else:\n",
        "                loss_count += 1\n",
        "                loss_points += ce_result['profit']\n",
        "\n",
        "            # Log trade details\n",
        "            trade_log.append({\n",
        "                \"Trade Type\": \"CE\",\n",
        "                \"Entry Datetime\": ce_result['entry_datetime'],\n",
        "                \"Exit Datetime\": ce_result['exit_datetime'],\n",
        "                \"Entry Price\": ce_result['entry_price'],\n",
        "                \"Exit Price\": ce_result['exit_price'],\n",
        "                \"Profit Points\": ce_result['profit'],\n",
        "                \"Status\": ce_result['status']\n",
        "            })\n",
        "\n",
        "            # Skip rows used during this trade\n",
        "            i += ce_result['rows_used']\n",
        "            continue\n",
        "\n",
        "        # If neither strategy triggers a trade, move to the next row\n",
        "        i += 1\n",
        "\n",
        "    # Convert trade log to DataFrame and save to Google Drive\n",
        "    trade_log_df = pd.DataFrame(trade_log)\n",
        "    trade_log_df.to_csv(trade_log_file, index=False)\n",
        "\n",
        "    print(f\"Trade details saved to Google Drive: {trade_log_file}\")\n",
        "\n",
        "    # Prepare summary results\n",
        "    results = {\n",
        "        'Total Trades': trade_count,\n",
        "        'Winning Trades': win_count,\n",
        "        'Losing Trades': loss_count,\n",
        "        'Win Rate (%)': (win_count / trade_count * 100) if trade_count > 0 else 0,\n",
        "        'Net Profit Points': net_profit,\n",
        "        'Win Points': win_points,\n",
        "        'Loss Points': loss_points,\n",
        "        'PE Trades': pe_count,\n",
        "        'CE Trades': ce_count\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(\"Backtest Results:\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the data from Google Drive\n",
        "    file_1min = \"/content/drive/My Drive/1min_data_2024.csv\"  # Update with actual path\n",
        "    data = pd.read_csv(file_1min, parse_dates=['date'])\n",
        "    data.set_index('date', inplace=True)\n",
        "\n",
        "    # Call the strategy execution\n",
        "    results = execute_strategy(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5FRvuOZ-YAy",
        "outputId": "805806b0-5fe2-4055-ae79-dc7b94789f3b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trade details saved to Google Drive: /content/drive/My Drive/trade_details.csv\n",
            "Backtest Results:\n",
            "Total Trades: 227\n",
            "Winning Trades: 101\n",
            "Losing Trades: 126\n",
            "Win Rate (%): 44.49339207048458\n",
            "Net Profit Points: 2954.0548099204752\n",
            "Win Points: 5965.0\n",
            "Loss Points: -3010.9451900795248\n",
            "PE Trades: 140\n",
            "CE Trades: 87\n"
          ]
        }
      ]
    }
  ]
}